name: Performance Benchmarks

on:
  push:
    branches: [ main, master, claude/* ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch: # Allow manual trigger

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        profile: minimal
        override: true

    - name: Cache Cargo dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Install Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y bc jq time

    - name: Build release binaries
      run: cargo build --release --all-targets

    - name: Build native benchmarks
      run: |
        chmod +x benchmarks/native/build.sh
        ./benchmarks/native/build.sh

    - name: Make benchmark scripts executable
      run: |
        chmod +x benchmarks/run_all.sh
        chmod +x benchmarks/run_with_tracking.sh
        chmod +x benchmarks/python/*.py

    - name: Run benchmarks with tracking
      run: ./benchmarks/run_with_tracking.sh

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: benchmarks/results/*.json

    - name: Display results summary
      run: |
        if [ -f benchmarks/results/bench_*.json ]; then
          LATEST=$(ls -t benchmarks/results/bench_*.json | head -1)
          echo "### Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
          cat "$LATEST" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY

          if command -v jq &> /dev/null; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Performance Summary" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            avg_total=$(jq '.benchmarks | to_entries | map(.value.speedups.nanbox_vs_interp) | add / length' "$LATEST")
            avg_vs_python=$(jq '.benchmarks | to_entries | map(select(.value.speedups.nanbox_vs_python > 0) | .value.speedups.nanbox_vs_python) | add / length' "$LATEST")

            echo "- **Total Speedup (NaN Boxing vs Interpreter):** ${avg_total}x" >> $GITHUB_STEP_SUMMARY
            echo "- **Performance vs Python:** ${avg_vs_python}x ($(echo "$avg_vs_python * 100" | bc)%)" >> $GITHUB_STEP_SUMMARY
          fi
        fi

    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const results = fs.readdirSync('benchmarks/results')
            .filter(f => f.startsWith('bench_'))
            .sort()
            .reverse()[0];

          if (results) {
            const data = JSON.parse(fs.readFileSync(`benchmarks/results/${results}`, 'utf8'));

            // Build performance table
            let comment = '## ðŸ“Š Performance Benchmark Results\n\n';
            comment += `**Commit:** \`${data.git_commit.substring(0, 7)}\`\n`;
            comment += `**Branch:** \`${data.git_branch}\`\n`;
            comment += `**Timestamp:** ${data.timestamp}\n\n`;

            // Main performance table
            comment += '### Execution Times\n\n';
            comment += '| Benchmark | Interpreter | Bytecode | NaN Boxing | Native (Rust) | Python | \n';
            comment += '|-----------|-------------|----------|------------|---------------|--------|\n';

            let hasPython = false;
            let hasNative = false;
            for (const [name, bench] of Object.entries(data.benchmarks)) {
              const nativeTime = bench.native ? `${bench.native.avg_ms}ms` : '-';
              const pythonTime = bench.python ? `${bench.python.avg_ms}ms` : '-';
              if (bench.native) hasNative = true;
              if (bench.python) hasPython = true;
              comment += `| **${name}** | ${bench.interpreter.avg_ms}ms | ${bench.bytecode.avg_ms}ms | ${bench.nanbox.avg_ms}ms | ${nativeTime} | ${pythonTime} |\n`;
            }

            // Speedup analysis table
            comment += '\n### Speedup Analysis\n\n';
            comment += '| Benchmark | Bytecode vs Interp | NanBox vs Bytecode | NanBox vs Interp | NanBox vs Native | NanBox vs Python |\n';
            comment += '|-----------|--------------------|--------------------|------------------|------------------|------------------|\n';

            for (const [name, bench] of Object.entries(data.benchmarks)) {
              const sp = bench.speedups;
              let vsNative = '-';
              if (sp.nanbox_vs_native > 0) {
                const percent = sp.nanbox_vs_native > 1
                  ? ((1 / sp.nanbox_vs_native) * 100).toFixed(1)
                  : (sp.nanbox_vs_native * 100).toFixed(1);
                vsNative = `${sp.nanbox_vs_native.toFixed(2)}x (${percent}%)`;
              }
              const vsPython = sp.nanbox_vs_python > 0 ? `${sp.nanbox_vs_python.toFixed(2)}x (${(sp.nanbox_vs_python * 100).toFixed(1)}%)` : '-';
              comment += `| **${name}** | ${sp.bytecode_vs_interp.toFixed(2)}x | ${sp.nanbox_vs_bytecode.toFixed(2)}x | ${sp.nanbox_vs_interp.toFixed(2)}x | ${vsNative} | ${vsPython} |\n`;
            }

            // Calculate averages
            const benchmarks = Object.values(data.benchmarks);
            const avgBytecodeSpeedup = (benchmarks.reduce((sum, b) => sum + b.speedups.bytecode_vs_interp, 0) / benchmarks.length).toFixed(2);
            const avgNanboxSpeedup = (benchmarks.reduce((sum, b) => sum + b.speedups.nanbox_vs_bytecode, 0) / benchmarks.length).toFixed(2);
            const avgTotalSpeedup = (benchmarks.reduce((sum, b) => sum + b.speedups.nanbox_vs_interp, 0) / benchmarks.length).toFixed(2);

            comment += '\n### Summary\n\n';
            comment += `- ðŸš€ **Bytecode optimization:** ${avgBytecodeSpeedup}x faster than interpreter\n`;
            comment += `- âš¡ **NaN Boxing optimization:** ${avgNanboxSpeedup}x faster than bytecode\n`;
            comment += `- ðŸŽ¯ **Total speedup:** ${avgTotalSpeedup}x faster than interpreter\n`;

            if (hasNative) {
              const nativeBenchmarks = benchmarks.filter(b => b.speedups.nanbox_vs_native > 0);
              if (nativeBenchmarks.length > 0) {
                const avgVsNative = (nativeBenchmarks.reduce((sum, b) => sum + b.speedups.nanbox_vs_native, 0) / nativeBenchmarks.length).toFixed(2);
                // If avgVsNative > 1, TopLang is slower, so invert for percentage
                const percentOfNative = avgVsNative > 1
                  ? ((1 / parseFloat(avgVsNative)) * 100).toFixed(1)
                  : (parseFloat(avgVsNative) * 100).toFixed(1);

                const emoji = percentOfNative >= 50 ? 'ðŸ†' : percentOfNative >= 20 ? 'âš¡' : 'ðŸ”§';
                const comparison = avgVsNative > 1 ? `${avgVsNative}x slower` : `${avgVsNative}x faster`;
                comment += `- ${emoji} **Performance vs Native Rust:** ${comparison} (${percentOfNative}% of native speed)\n`;
              }
            }

            if (hasPython) {
              const pythonBenchmarks = benchmarks.filter(b => b.speedups.nanbox_vs_python > 0);
              if (pythonBenchmarks.length > 0) {
                const avgVsPython = (pythonBenchmarks.reduce((sum, b) => sum + b.speedups.nanbox_vs_python, 0) / pythonBenchmarks.length).toFixed(2);
                const percentOfPython = (parseFloat(avgVsPython) * 100).toFixed(1);

                const emoji = percentOfPython >= 80 ? 'ðŸ†' : percentOfPython >= 50 ? 'âš¡' : 'ðŸ”§';
                comment += `- ${emoji} **Performance vs Python:** ${avgVsPython}x (${percentOfPython}% of Python's speed)\n`;
              }
            }

            comment += `\n---\n*Benchmark ran ${data.runs_per_benchmark} times per configuration. Results show average execution time.*`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  benchmark-comparison:
    name: Historical Comparison
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'push'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for comparison

    - name: Download current results
      uses: actions/download-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: benchmarks/results/current

    - name: Download previous results
      uses: actions/download-artifact@v4
      continue-on-error: true
      with:
        name: benchmark-results
        path: benchmarks/results/previous

    - name: Compare results
      run: |
        if [ -d benchmarks/results/previous ]; then
          echo "Comparing with previous benchmark run..."
          # TODO: Add comparison logic here
          echo "Comparison not yet implemented"
        else
          echo "No previous results found for comparison"
        fi
