name: Performance Benchmarks

on:
  push:
    branches: [ main, master, claude/* ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch: # Allow manual trigger

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        profile: minimal
        override: true

    - name: Cache Cargo dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Install Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y bc jq time

    - name: Build release binaries
      run: cargo build --release --all-targets

    - name: Make benchmark scripts executable
      run: |
        chmod +x benchmarks/run_all.sh
        chmod +x benchmarks/run_with_tracking.sh
        chmod +x benchmarks/python/*.py

    - name: Run benchmarks with tracking
      run: ./benchmarks/run_with_tracking.sh

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: benchmarks/results/*.json

    - name: Display results summary
      run: |
        if [ -f benchmarks/results/bench_*.json ]; then
          LATEST=$(ls -t benchmarks/results/bench_*.json | head -1)
          echo "### ðŸ“Š Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if command -v jq &> /dev/null; then
            echo "| Benchmark | Interpreter | VM | Native | Native Speedup |" >> $GITHUB_STEP_SUMMARY
            echo "|-----------|-------------|-----|--------|----------------|" >> $GITHUB_STEP_SUMMARY

            jq -r '.benchmarks | to_entries[] | "| \(.key) | \(.value.interpreter.avg_ms)ms | \(.value.nanbox.avg_ms)ms | \(.value.native.avg_ms)ms | **\(.value.speedups.native_vs_interp | tonumber | . * 10 | round / 10)x** âš¡ |"' "$LATEST" >> $GITHUB_STEP_SUMMARY

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Performance Summary" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            avg_vm=$(jq '.benchmarks | to_entries | map(.value.speedups.nanbox_vs_interp) | add / length' "$LATEST")
            avg_native=$(jq '.benchmarks | to_entries | map(.value.speedups.native_vs_interp) | add / length' "$LATEST")
            avg_compile=$(jq '.benchmarks | to_entries | map(.value.native.compile_ms) | add / length' "$LATEST")

            echo "- **VM Speedup (NaN Boxing vs Interpreter):** ${avg_vm}x" >> $GITHUB_STEP_SUMMARY
            echo "- **Native Speedup vs Interpreter:** ${avg_native}x ðŸš€" >> $GITHUB_STEP_SUMMARY
            echo "- **Average Compilation Time:** ${avg_compile}ms (very fast!)" >> $GITHUB_STEP_SUMMARY
          fi
        fi

    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const results = fs.readdirSync('benchmarks/results')
            .filter(f => f.startsWith('bench_'))
            .sort()
            .reverse()[0];

          if (results) {
            const data = JSON.parse(fs.readFileSync(`benchmarks/results/${results}`, 'utf8'));

            let comment = '## ðŸ“Š Benchmark Results\n\n';
            comment += '| Benchmark | Interpreter | VM (NanBox) | Native (AOT) | Native Speedup | Compile Time |\n';
            comment += '|-----------|-------------|-------------|--------------|----------------|---------------|\n';

            for (const [name, bench] of Object.entries(data.benchmarks)) {
              const nativeSpeedup = bench.speedups.native_vs_interp.toFixed(1);
              const compileTime = bench.native.compile_ms;
              comment += `| ${name} | ${bench.interpreter.avg_ms}ms | ${bench.nanbox.avg_ms}ms | ${bench.native.avg_ms}ms | **${nativeSpeedup}x** âš¡ | ${compileTime}ms |\n`;
            }

            // Add summary
            const benchmarks = Object.values(data.benchmarks);
            const avgNativeSpeedup = (benchmarks.reduce((sum, b) => sum + b.speedups.native_vs_interp, 0) / benchmarks.length).toFixed(1);
            const avgCompileTime = Math.round(benchmarks.reduce((sum, b) => sum + b.native.compile_ms, 0) / benchmarks.length);

            comment += '\n**Summary:**\n';
            comment += `- Average native speedup: **${avgNativeSpeedup}x faster** than interpreter ðŸš€\n`;
            comment += `- Average compilation time: ${avgCompileTime}ms (very fast!)\n`;
            comment += `- System: ${data.system.os} ${data.system.arch}\n`;
            comment += `- Commit: ${data.git_commit.substring(0, 7)}\n`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  benchmark-comparison:
    name: Historical Comparison
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'push'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for comparison

    - name: Download current results
      uses: actions/download-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: benchmarks/results/current

    - name: Download previous results
      uses: actions/download-artifact@v4
      continue-on-error: true
      with:
        name: benchmark-results
        path: benchmarks/results/previous

    - name: Compare results
      run: |
        if [ -d benchmarks/results/previous ]; then
          echo "Comparing with previous benchmark run..."
          # TODO: Add comparison logic here
          echo "Comparison not yet implemented"
        else
          echo "No previous results found for comparison"
        fi
